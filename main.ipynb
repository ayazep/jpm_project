{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\JackHarrigan\\AppData\\Local\\Temp\\ipykernel_35912\\3461912108.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = pd.read_csv(filepath, sep = '\\s+', header = None, names = column_names)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = r'.\\data\\housing.csv'\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv(filepath, sep = '\\s+', header = None, names = column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 7, figsize = (15, 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.boxplot(y = df[column], data = df, ax = axes[i], color = '#14F278')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plots\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 7, figsize = (15, 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.regplot(x = df[column], y = df['MEDV'], data = df, ax = axes[i], color = '#14F278', line_kws = dict(color = '#064824'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "pairplot = sns.pairplot(df, kind = 'reg', plot_kws={'color': '#14F278', 'line_kws': {'color': '#064824'}}, diag_kws={'color': '#14F278'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wanted to explore MEDV further as it is our target value. We noticed there for RM vs MEDV that there are a few towns with the same MEDV but very different RM values.\n",
    "# After further investigation we learned that every MEDV value above 50 was rounded down to 50, so we decided to remove those rows.\n",
    "df = df[~(df['MEDV'] >= 50.0)].reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "corr_df = df.corr().abs()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "sns.heatmap(corr_df, annot = True, cmap = 'BuGn', linewidths = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance Heatmap\n",
    "cov_df = df.cov()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "sns.heatmap(cov_df, annot = True, cmap = 'BuGn', linewidths = 0.5, center = 0, robust = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "X = df.drop(columns = ['MEDV'])\n",
    "y = df['MEDV']\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,  test_size = 0.20, random_state = 42)\n",
    " \n",
    "model = Lasso(alpha = 1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred1 = model.predict(x_test)\n",
    " \n",
    "lasso = pd.DataFrame()\n",
    "lasso['Columns'] = x_train.columns\n",
    "lasso['Estimate'] = pd.Series(model.coef_)\n",
    " \n",
    "sns.barplot(x = lasso['Columns'], y = lasso['Estimate'], color = '#14F278')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model - Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees (GBT) is a popular machine learning technique used for both regression and classification tasks. It's an ensamble learning method, meaning it combines the predictions of another model to improve overall performance.\n",
    "\n",
    "Gradient boosting is a sequential technique where new models are added to correct the errors made by existing models. In each iteration, a new decision tree model is trained to predict the residuals (the differences between the actual values and the predictions of the ensemble so far).\n",
    "\n",
    "A decision tree is a flowchart-like structure where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents the outcome or prediction.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the gradient descent optimization algorithm. It's used to minimize the loss function of the ensemble model. Gradient descent adjusts the parameters of the new model in the direction that reduces the loss the most.\n",
    "\n",
    "Each new decision tree model is added to the ensemble, with its predictions weighted by a factor (learning rate) that determines how much influence it has on the final prediction. The learning rate is typically a small value, and it helps to prevent overfitting and stabilize the training process.\n",
    "\n",
    "The process of adding new models continues until a predefined number of trees are added or until a stopping criterion is met, such as the maximum depth of the tree or the minimum number of samples in each node.\n",
    "\n",
    "Overall, Gradient Boosted Trees are powerful and widely used due to their ability to capture complex relationships in data, handle missing values, and provide high predictive accuracy.\n",
    "\n",
    "\n",
    "Based on our EDA, we've decided to use RM, PTRATIO, TAX, and LSTAT as the features to predict the MEDV values.\n",
    "\n",
    "- RM:\n",
    "- PTRATIO:\n",
    "- TAX:\n",
    "- LSTAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPTRATIO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTAT\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df[['RM', 'PTRATIO', 'TAX', 'LSTAT']]\n",
    "y = df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Initialize an instance of a Gradient Boosting Regression\n",
    "model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning algorithm to optimize its performance on a give dataset.\n",
    "\n",
    "To find the best set of hyperparameters for our model, we decided to use GridSearchCV. GridSearchCV (Grid Search Cross-Validation) searches through a manually specified subset of hyperparameter combinations to find the best set for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass Ranges for Hyperparameters\n",
    "parameters = {'max_depth': [2, 3, 4],\n",
    "              'min_samples_split': [4, 5, 6],\n",
    "              'learning_rate': [0.05, 0.1, 0.2],\n",
    "              'n_estimators': [25, 50, 75]}  \n",
    "\n",
    "# Initialise a Grid Search object\n",
    "clf = GridSearchCV(estimator = model, param_grid = parameters, cv = 5)\n",
    "\n",
    "# Fit the Grid Search object to the train data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the optimal hyperparamer values, found by Grid Search CV\n",
    "best_depth = clf.best_estimator_.max_depth\n",
    "best_min_samples = clf.best_estimator_.min_samples_leaf\n",
    "\n",
    "print(f'best estimator: depth {best_depth} and min_samples {best_min_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean absolute error is {mae:.4f}')\n",
    "print(f'MSE (test): {mse:.4f}')\n",
    "print(f'RMSE (test): {mse**0.5:.4f}')\n",
    "print(f'R-squared is {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model - From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"This class defines the attributes of a tree node.\"\"\"\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, value = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"Define a decision tree regressor.\n",
    "    \n",
    "    Args:\n",
    "        min_samples (int, optional): The minimum number of samples required to split an\n",
    "        internal node. Defaults to 5.\n",
    "        max_depth (int, optional): The maximum depth of the tree. Defaults to 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples: int = 5, max_depth: int = 3):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        X = X.to_numpy()\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> list:\n",
    "        \"\"\"Predict the y values.\"\"\"\n",
    "        X = X.to_numpy()\n",
    "        return [self._transverse(x, self.root) for x in X]\n",
    "\n",
    "    def _transverse(self, x: np.array, node: Node) -> float:\n",
    "        \"\"\"Retrieve the value of y.\"\"\"\n",
    "        if node.value is None:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return self._transverse(x, node.left)\n",
    "            return self._transverse(x, node.right)\n",
    "        return node.value\n",
    "\n",
    "    def _grow_tree(self, X: np.array, y: np.array, depth: int = 0) -> Node:\n",
    "        \"\"\"Split the data into leaf nodes.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        if n_samples >= self.min_samples and depth <= self.max_depth:\n",
    "            index, value = self._best_split(X, y)\n",
    "            left_mask = X[:, index] <= value\n",
    "            right_mask = X[:, index] > value\n",
    "\n",
    "            left = self._grow_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right = self._grow_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "            return Node(feature = index, threshold = value, left = left, right = right)\n",
    "\n",
    "        return Node(value = self._leaf_node(y))\n",
    "\n",
    "    def _best_split(self, X: pd.DataFrame, y: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Find the best threshold and index to split the node on.\"\"\"\n",
    "        best_rss = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_i in range(n_features):\n",
    "            for threshold in np.unique(X[:, feature_i]):\n",
    "                left_mask = X[:, feature_i] <= threshold\n",
    "                right_mask = X[:, feature_i] > threshold\n",
    "                rss = self._rss(y, left_mask, right_mask)\n",
    "                if rss < best_rss:\n",
    "                    best_rss = rss\n",
    "                    best_feature = feature_i\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _rss(self, y: pd.DataFrame, left, right) -> float:\n",
    "        \"\"\"Calculate the Residual Sum of Squares (RSS).\"\"\"\n",
    "        y_left = y[left]\n",
    "        y_right = y[right]\n",
    "        rss_left = np.sum((y_left - np.mean(y_left)) ** 2)\n",
    "        rss_right = np.sum((y_right - np.mean(y_right)) ** 2)\n",
    "        return rss_left + rss_right\n",
    "\n",
    "    def _leaf_node(self, y: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate the value of a leaf node.\"\"\"\n",
    "        return np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchGradientBoostingRegressor():\n",
    "    \"\"\"Define a gradient boosting regressor.\"\"\"\n",
    "    def __init__(self, max_depth = 3, min_samples = 5, learning_rate = 0.1, n_estimators = 50):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = DecisionTree(min_samples = self.min_samples, max_depth = self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        \"\"\"Build a gradient boosting regressor from the training set (X, y).\"\"\"\n",
    "        y_pred = np.copy(y)\n",
    "        for tree in self.trees:\n",
    "            tree.fit(X, y_pred)\n",
    "            new_y_pred = tree.predict(X)\n",
    "            y_pred = y_pred - np.multiply(self.learning_rate, new_y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the y values.\"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            new_y_pred = tree.predict(X)\n",
    "            y_pred += np.multiply(self.learning_rate, new_y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metrics\n",
    "We calculated a few the basic performance metrics from scratch to evaluate our model with\n",
    "including Mean Absolute Error, Mean Squared Error, and The sum of the squared residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_val(y, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
    "    return np.mean(abs(y - y_pred))\n",
    "\n",
    "def mse_val(y, y_pred):\n",
    "    \"\"\"Calculate Mean Square Error.\"\"\"\n",
    "    return np.mean(np.square(np.subtract(y, y_pred)))\n",
    "\n",
    "def r_square(y, y_pred):\n",
    "    \"\"\"Calculate R-Squared.\"\"\"\n",
    "    ssr = np.sum(np.square(np.subtract(y, y_pred)))\n",
    "    sst = np.sum(np.square(np.subtract(y, np.mean(y))))\n",
    "    return 1 - ssr/sst\n",
    "\n",
    "def performance_evaluation(y, y_pred):\n",
    "    \"\"\"Print all the performance metrics.\"\"\"\n",
    "    mae = mae_val(y, y_pred)\n",
    "    mse = mse_val(y, y_pred)\n",
    "    r2 = r_square(y, y_pred)\n",
    "\n",
    "    print(f'Mean absolute error is {mae:.4f}')\n",
    "    print(f'MSE (test): {mse:.4f}')\n",
    "    print(f'RMSE (test): {mse**0.5:.4f}')\n",
    "    print(f'R-squared is {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our version of the sklearn train_test_split method. We included a random seed so that you can reproduce the same random\n",
    "dataset for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y, train_size: float = 0.8, seed: int = None):\n",
    "    \"\"\"Split the dataset into train and tests sets.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    num_samples = len(X)\n",
    "    index = np.arange(num_samples)\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    X = X.loc[index]\n",
    "    y = y.loc[index]\n",
    "\n",
    "    split_i = int(num_samples * train_size)\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Fold\n",
    "\n",
    "The k-fold method is a tool used in hyperparameter tuning that allows you to split up the data into multiple subsets or \"folds\" and then treat each individual subset\n",
    "as the validation, or testing set. The other subsets are treated as the trainning set for the model to fit on. This method is useful as it reduces the variance and bias\n",
    "of the predicted target values. It also accounts for the variability of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(model, X, y, n_folds, seed: int = None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    index = list(np.arange(len(X)))\n",
    "    np.random.shuffle(index)\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        marker = int(len(X)/n_folds)\n",
    "        testing_index = index[marker * i : marker * (i+1)]\n",
    "        training_index = index[:marker * i] + index[marker * (i+1):]\n",
    "        if i == (n_folds - 1):\n",
    "            testing_index += index[marker * (i+1):]\n",
    "            training_index = training_index[:marker* i]\n",
    "\n",
    "        X_train = X.loc[training_index]\n",
    "        X_test = X.loc[testing_index]\n",
    "        y_train = y.loc[training_index]\n",
    "        y_test = y.loc[testing_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r_square(y_test, y_pred)\n",
    "        results.append(r2)\n",
    "    \n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of Grid Search hyperparameter tuning method finds all of the possible combinations of the hyperparameters \n",
    "and then runs the model on all of the different combinations to find the optimal combination that produces the most positive\n",
    "results. There is also a input parameter for the number of folds to be used if you want to utilize k-fold cross validation. \n",
    "Using this function we found that the optimal paramters for our gradient boosting model were a minimum sample split size of 6, \n",
    "a maximum decision tree depth of 3, a learning rate of 0.2, and the number of tree estimators to be 50. These parameters gave us a\n",
    "r2 score of 0.86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(parameters, X, y, n_folds:int = 0, seed: int = None, train_size: float = 0.8):\n",
    "    X_train, X_test, y_train, y_test = train_test(X,y, train_size, seed)\n",
    "\n",
    "    best_r2 = 0\n",
    "    param_combinations = list(itertools.product(*parameters.values()))\n",
    "\n",
    "    for combo in param_combinations:\n",
    "        model = ScratchGradientBoostingRegressor(**dict(zip(parameters.keys(), combo)))\n",
    "\n",
    "        r2 = 0\n",
    "        if n_folds > 0:\n",
    "            r2 = k_fold(model, X, y, n_folds, seed)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2 = r_square(y_test, y_pred)\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_combo = combo\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "    return best_r2, best_combo, best_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2, 3, 4],\n",
    "              'min_samples': [4, 5, 6],\n",
    "              'learning_rate': [0.05, 0.1, 0.2],\n",
    "              'n_estimators': [25, 50, 75]} \n",
    "\n",
    "best_r2, best_params, y_pred = grid_search(parameters, X, y, 0, 42, 0.8)\n",
    "\n",
    "performance_evaluation(y_test, y_pred)\n",
    "sns.scatterplot(x = y_test, y = y_pred)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_evaluation(y_test, y_pred)\n",
    "\n",
    "sns.scatterplot(x = y_test, y = y_pred, color = '#14F278')\n",
    "\n",
    "plt.xlabel(\"Actual Values (y_test)\")\n",
    "plt.ylabel(\"Predicted Values (y_pred)\")\n",
    "plt.title(\"Scatter Plot of Actual vs. Predicted Values\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
