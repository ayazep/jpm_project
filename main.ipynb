{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'.\\data\\housing.csv'\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv(filepath, sep = '\\s+', header = None, names = column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 7, figsize = (15, 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.boxplot(y = df[column], data = df, ax = axes[i], color = '#14F278')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plots\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 7, figsize = (15, 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.regplot(x = df[column], y = df['MEDV'], data = df, ax = axes[i], color = '#14F278', line_kws = dict(color = '#064824'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "pairplot = sns.pairplot(df, kind = 'reg', plot_kws={'color': '#14F278', 'line_kws': {'color': '#064824'}}, diag_kws={'color': '#14F278'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wanted to explore MEDV further as it is our target value. We noticed there for RM vs MEDV that there are a few towns with the same MEDV but very different RM values.\n",
    "# After further investigation we learned that every MEDV value above 50 was rounded down to 50, so we decided to remove those rows.\n",
    "df = df[~(df['MEDV'] >= 50.0)].reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "corr_df = df.corr().abs()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "sns.heatmap(corr_df, annot = True, cmap = 'BuGn', linewidths = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance Heatmap\n",
    "cov_df = df.cov()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "sns.heatmap(cov_df, annot = True, cmap = 'BuGn', linewidths = 0.5, center = 0, robust = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "X = df.drop(columns = ['MEDV'])\n",
    "y = df['MEDV']\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,  test_size = 0.20, random_state = 42)\n",
    " \n",
    "model = Lasso(alpha = 1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred1 = model.predict(x_test)\n",
    " \n",
    "lasso = pd.DataFrame()\n",
    "lasso['Columns'] = x_train.columns\n",
    "lasso['Estimate'] = pd.Series(model.coef_)\n",
    " \n",
    "sns.barplot(x = lasso['Columns'], y = lasso['Estimate'], color = '#14F278')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model - Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees (GBT) is a popular machine learning technique used for both regression and classification tasks. It's an ensamble learning method, meaning it combines the predictions of another model to improve overall performance.\n",
    "\n",
    "Gradient boosting is a sequential technique where new models are added to correct the errors made by existing models. In each iteration, a new decision tree model is trained to predict the residuals (the differences between the actual values and the predictions of the ensemble so far).\n",
    "\n",
    "A decision tree is a flowchart-like structure where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents the outcome or prediction.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the gradient descent optimization algorithm. It's used to minimize the loss function of the ensemble model. Gradient descent adjusts the parameters of the new model in the direction that reduces the loss the most.\n",
    "\n",
    "Each new decision tree model is added to the ensemble, with its predictions weighted by a factor (learning rate) that determines how much influence it has on the final prediction. The learning rate is typically a small value, and it helps to prevent overfitting and stabilize the training process.\n",
    "\n",
    "The process of adding new models continues until a predefined number of trees are added or until a stopping criterion is met, such as the maximum depth of the tree or the minimum number of samples in each node.\n",
    "\n",
    "Overall, Gradient Boosted Trees are powerful and widely used due to their ability to capture complex relationships in data, handle missing values, and provide high predictive accuracy.\n",
    "\n",
    "\n",
    "Based on our EDA, we've decided to use RM, PTRATIO, TAX, and LSTAT as the features to predict the MEDV values.\n",
    "\n",
    "- RM:\n",
    "- PTRATIO:\n",
    "- TAX:\n",
    "- LSTAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['RM', 'PTRATIO', 'TAX', 'LSTAT']]\n",
    "y = df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Initialize an instance of a Gradient Boosting Regression\n",
    "model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning algorithm to optimize its performance on a give dataset.\n",
    "\n",
    "To find the best set of hyperparameters for our model, we decided to use GridSearchCV. GridSearchCV (Grid Search Cross-Validation) searches through a manually specified subset of hyperparameter combinations to find the best set for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass Ranges for Hyperparameters\n",
    "parameters = {'max_depth': [2, 3, 4],\n",
    "              'min_samples_split': [4, 5, 6],\n",
    "              'learning_rate': [0.05, 0.1, 0.2],\n",
    "              'n_estimators': [25, 50, 75]}  \n",
    "\n",
    "# Initialise a Grid Search object\n",
    "clf = GridSearchCV(estimator = model, param_grid = parameters, cv = 5)\n",
    "\n",
    "# Fit the Grid Search object to the train data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the optimal hyperparamer values, found by Grid Search CV\n",
    "best_depth = clf.best_estimator_.max_depth\n",
    "best_min_samples = clf.best_estimator_.min_samples_leaf\n",
    "\n",
    "print(f'best estimator: depth {best_depth} and min_samples {best_min_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean absolute error is {mae:.4f}')\n",
    "print(f'MSE (test): {mse:.4f}')\n",
    "print(f'RMSE (test): {mse**0.5:.4f}')\n",
    "print(f'R-squared is {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model - From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"This class defines the attributes of a tree node.\"\"\"\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, value = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"Define a decision tree regressor.\n",
    "    \n",
    "    Args:\n",
    "        min_samples (int, optional): The minimum number of samples required to split an\n",
    "        internal node. Defaults to 5.\n",
    "        max_depth (int, optional): The maximum depth of the tree. Defaults to 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples: int = 5, max_depth: int = 3):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        X = X.to_numpy()\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> list:\n",
    "        \"\"\"Predict the y values.\"\"\"\n",
    "        X = X.to_numpy()\n",
    "        return [self._transverse(x, self.root) for x in X]\n",
    "\n",
    "    def _transverse(self, x: np.array, node: Node) -> float:\n",
    "        \"\"\"Retrieve the value of y.\"\"\"\n",
    "        if node.value is None:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return self._transverse(x, node.left)\n",
    "            return self._transverse(x, node.right)\n",
    "        return node.value\n",
    "\n",
    "    def _grow_tree(self, X: np.array, y: np.array, depth: int = 0) -> Node:\n",
    "        \"\"\"Split the data into leaf nodes.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        if n_samples >= self.min_samples and depth <= self.max_depth:\n",
    "            index, value = self._best_split(X, y)\n",
    "            left_mask = X[:, index] <= value\n",
    "            right_mask = X[:, index] > value\n",
    "\n",
    "            left = self._grow_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right = self._grow_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "            return Node(feature = index, threshold = value, left = left, right = right)\n",
    "\n",
    "        return Node(value = self._leaf_node(y))\n",
    "\n",
    "    def _best_split(self, X: pd.DataFrame, y: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Find the best threshold and index to split the node on.\"\"\"\n",
    "        best_rss = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_i in range(n_features):\n",
    "            for threshold in np.unique(X[:, feature_i]):\n",
    "                left_mask = X[:, feature_i] <= threshold\n",
    "                right_mask = X[:, feature_i] > threshold\n",
    "                rss = self._rss(y, left_mask, right_mask)\n",
    "                if rss < best_rss:\n",
    "                    best_rss = rss\n",
    "                    best_feature = feature_i\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _rss(self, y: pd.DataFrame, left, right) -> float:\n",
    "        \"\"\"Calculate the Residual Sum of Squares (RSS).\"\"\"\n",
    "        y_left = y[left]\n",
    "        y_right = y[right]\n",
    "        rss_left = np.sum((y_left - np.mean(y_left)) ** 2)\n",
    "        rss_right = np.sum((y_right - np.mean(y_right)) ** 2)\n",
    "        return rss_left + rss_right\n",
    "\n",
    "    def _leaf_node(self, y: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate the value of a leaf node.\"\"\"\n",
    "        return np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchGradientBoostingRegressor():\n",
    "    \"\"\"Define a gradient boosting regressor.\"\"\"\n",
    "    def __init__(self, max_depth = 3, min_samples = 5, learning_rate = 0.1, n_estimators = 50):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = DecisionTree(min_samples = self.min_samples, max_depth = self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        \"\"\"Build a gradient boosting regressor from the training set (X, y).\"\"\"\n",
    "        y_pred = np.copy(y)\n",
    "        for tree in self.trees:\n",
    "            tree.fit(X, y_pred)\n",
    "            new_y_pred = tree.predict(X)\n",
    "            y_pred = y_pred - np.multiply(self.learning_rate, new_y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the y values.\"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            new_y_pred = tree.predict(X)\n",
    "            y_pred += np.multiply(self.learning_rate, new_y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_val(y, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
    "    return np.mean(abs(y - y_pred))\n",
    "\n",
    "def mse_val(y, y_pred):\n",
    "    \"\"\"Calculate Mean Square Error.\"\"\"\n",
    "    return np.mean(np.square(np.subtract(y, y_pred)))\n",
    "\n",
    "def r_square(y, y_pred):\n",
    "    \"\"\"Calculate R-Squared.\"\"\"\n",
    "    ssr = np.sum(np.square(np.subtract(y, y_pred)))\n",
    "    sst = np.sum(np.square(np.subtract(y, np.mean(y))))\n",
    "    return 1 - ssr/sst\n",
    "\n",
    "def performance_evaluation(y, y_pred):\n",
    "    \"\"\"Print all the performance metrics.\"\"\"\n",
    "    mae = mae_val(y, y_pred)\n",
    "    mse = mse_val(y, y_pred)\n",
    "    r2 = r_square(y, y_pred)\n",
    "\n",
    "    print(f'Mean absolute error is {mae:.4f}')\n",
    "    print(f'MSE (test): {mse:.4f}')\n",
    "    print(f'RMSE (test): {mse**0.5:.4f}')\n",
    "    print(f'R-squared is {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y, train_size: float = 0.8, seed: int = None):\n",
    "    \"\"\"Split the dataset into train and tests sets.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    num_samples = len(X)\n",
    "    index = np.arange(num_samples)\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    X = X.loc[index]\n",
    "    y = y.loc[index]\n",
    "\n",
    "    split_i = int(num_samples * train_size)\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(parameters, X_train, X_test, y_train, y_test):\n",
    "    best_r2 = 0\n",
    "\n",
    "    param_combinations = list(itertools.product(*parameters.values()))\n",
    "\n",
    "    for combo in param_combinations:\n",
    "        model = ScratchGradientBoostingRegressor(**dict(zip(parameters.keys(), combo)))\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r_square(y_test, y_pred)\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_combo = combo\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "    return best_r2, best_combo, best_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test(X, y, train_size = 0.8, seed = 42)\n",
    "\n",
    "parameters = {'max_depth': [2, 3, 4],\n",
    "              'min_samples': [4, 5, 6],\n",
    "              'learning_rate': [0.05, 0.1, 0.2],\n",
    "              'n_estimators': [25, 50, 75]} \n",
    "\n",
    "best_r2, best_params, y_pred = grid_search(parameters, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_evaluation(y_test, y_pred)\n",
    "\n",
    "sns.scatterplot(x = y_test, y = y_pred, color = '#14F278')\n",
    "\n",
    "plt.xlabel(\"Actual Values (y_test)\")\n",
    "plt.ylabel(\"Predicted Values (y_pred)\")\n",
    "plt.title(\"Scatter Plot of Actual vs. Predicted Values\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
