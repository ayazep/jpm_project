{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JP Morgan - Boston Housing Analysis - Team Yin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "This dataset originates from a 1976 study, \"Hedonic Housing Prices and the Demand for Clean Air\" by two Cambridge-based researchers, David Harrison Jr and Daniel Rubenfeld. The original study was designed to measure the willingness of the Boston populace to pay for clean air, using the median home value of towns and suburbs in the Boston area to measure this willingness. Other variables are used to structure the data, as well as to give context to the neighbourhoods and suburbs that this data describes.\n",
    "\n",
    "The dataset consists of 506 rows, representing various neighbourhoods and suburbs in the Boston area. There are 14 attributes that each of these areas are measured on, with the final column, MEDV (median value of owner-occupied homes, in $10,000s) serving as the dependent variable. MEDV is truncated at 50, meaning that any areas with a median owner-occupied home value of at $500,000 or greater is listed as having a median value of $500,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhousing.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCRIM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINDUS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHAS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAGE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDIS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPTRATIO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(filepath, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, names \u001b[38;5;241m=\u001b[39m column_names)\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "filepath = r'.\\data\\housing.csv'\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv(filepath, sep = '\\s+', header = None, names = column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CRIM: the crime rate by town\n",
    "- ZN: the proportion of residential land in a town that is zoned for lots greater than 25,000 ft\n",
    "- INDUS: the proportion of land in a town that is non-retail business, by acre\n",
    "- CHAS: a binary value, where 0 indicates that the area does not bound the Charles River and where 1 indicated that the area does bound the Charles River\n",
    "- NOX: nitrogen oxide concentrations in parts per hundred million (pphm)\n",
    "- RM: the average number of rooms per owner-occupied home.\n",
    "- AGE: the proportion of owner-occupied homes built prior to 1940.\n",
    "- DIS: a weighted value that represents distance to 5 major employment centres in the Boston area, measured logarithmically \n",
    "- RAD: accessibility to radial highways, by town, measured logarithmically\n",
    "- TAX: The full-value property tax rate, in $10,000s\n",
    "- PTRATIO: The ratio of students to teachers per school district\n",
    "- B: the proportion of the area population that is Black, calculated by the formula 1000(B_k - 0.63)^2k)\n",
    "- LSTAT: the proportion of the population that is “lower status”, calculated by 0.5^(lower status proportion) \n",
    "\n",
    "\n",
    "#### Data Quality and Ethics\n",
    "\n",
    "Because of its origin point as a research study, this dataset has several data quality and data ethics issues to consider. \n",
    "\n",
    "1. Several columns are already transformed values\n",
    "2. Attempts to confirm data validity were unable to fully map the dataset back to the 1970 US Census\n",
    "3. Harrison and Rubenfeld assume a parabolic relaationship between MEDV and B, based on their belief that racial segregation increases housing values. They tranformed this data to fit this assumption, using 63% as the threshold where prices flip from declining to increasing. They did not provide any evidence to support this decision.\n",
    "4. LSTAT was created by taking the number of \"low status\" individuals, which are defined as people who did not graduate high school or male laborers. This value is transformed logarithmically in a manner that values distinctions between higher social classes more than lower ones.\n",
    "5. Most of this data is from the 1970 US census. Until the passing of the Fair Housing Act of 1968, it was legal to discriminate against potential renters and homeowners on the basis of race, religion, national origin, and sex. Use of this data to predict housing values runs the risk of reinforcing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graphs show different representations of the raw data to support us in identifying outliers and any patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 7, figsize = (15, 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.boxplot(y = df[column], data = df, ax = axes[i], color = '#14F278')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plots\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 7, figsize = (15, 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.regplot(x = df[column], y = df['MEDV'], data = df, ax = axes[i], color = '#14F278', line_kws = dict(color = '#064824'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "pairplot = sns.pairplot(df, kind = 'reg', plot_kws={'color': '#14F278', 'line_kws': {'color': '#064824'}}, diag_kws={'color': '#14F278'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wanted to explore MEDV further as it is our target value. We noticed there for RM vs MEDV that there are a few towns with the same MEDV but very different RM values.\n",
    "# After further investigation we learned that every MEDV value above 50 was rounded down to 50, so we decided to remove those rows.\n",
    "df = df[~(df['MEDV'] >= 50.0)].reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Columns\n",
    "\n",
    "The correlation heatmap calculates the correlation between the two intersecting variables, and is colored using the absolute value of the correlation. A deeper green indicates a stronger correlation, both negative and positive. MEDV is strongly correlated with LSTAT and RM, and also shows fairly strong correlation with CRIM, INDUS, NOX, AGE, RAD, TAX, and PTRATIO. Based on this matrix, ZN, CHAS, DIS, and B are not strong features for the model, as they do not have a strong relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "corr_df = df.corr().abs()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "sns.heatmap(corr_df, annot = True, cmap = 'BuGn', linewidths = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicoariance is a concern for the model features, as some of these features do depend on each other somewhat. We seek to ### FINISH SENTENCE\n",
    "Here, a darker green indicates stronger covariance. As covariance does depend on the size of the values used to calculate, this is not a metric we should use to exclude features. However, using it in tandem with the correlation matrix, it can draw our attention to certain featues that may be used to explain each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance Heatmap\n",
    "cov_df = df.cov()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "sns.heatmap(cov_df, annot = True, cmap = 'BuGn', linewidths = 0.5, center = 0, robust = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "X = df.drop(columns = ['MEDV'])\n",
    "y = df['MEDV']\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,  test_size = 0.20, random_state = 42)\n",
    " \n",
    "model = Lasso(alpha = 1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred1 = model.predict(x_test)\n",
    " \n",
    "lasso = pd.DataFrame()\n",
    "lasso['Columns'] = x_train.columns\n",
    "lasso['Estimate'] = pd.Series(model.coef_)\n",
    " \n",
    "sns.barplot(x = lasso['Columns'], y = lasso['Estimate'], color = '#14F278')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model - Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees (GBT) is a popular machine learning technique used for both regression and classification tasks. It's an ensamble learning method, meaning it combines the predictions of another model to improve overall performance.\n",
    "\n",
    "Gradient boosting is a sequential technique where new models are added to correct the errors made by existing models. In each iteration, a new decision tree model is trained to predict the residuals (the differences between the actual values and the predictions of the ensemble so far).\n",
    "\n",
    "A decision tree is a flowchart-like structure where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents the outcome or prediction.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the gradient descent optimization algorithm. It's used to minimize the loss function of the ensemble model. Gradient descent adjusts the parameters of the new model in the direction that reduces the loss the most.\n",
    "\n",
    "Each new decision tree model is added to the ensemble, with its predictions weighted by a factor (learning rate) that determines how much influence it has on the final prediction. The learning rate is typically a small value, and it helps to prevent overfitting and stabilize the training process.\n",
    "\n",
    "The process of adding new models continues until a predefined number of trees are added or until a stopping criterion is met, such as the maximum depth of the tree or the minimum number of samples in each node.\n",
    "\n",
    "Overall, Gradient Boosted Trees are powerful and widely used due to their ability to capture complex relationships in data, handle missing values, and provide high predictive accuracy.\n",
    "\n",
    "\n",
    "Based on our EDA, we've decided to use RM, PTRATIO, TAX, and LSTAT as the features to predict the MEDV values.\n",
    "\n",
    "- RM:\n",
    "- PTRATIO:\n",
    "- TAX:\n",
    "- LSTAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPTRATIO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTAT\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df[['RM', 'PTRATIO', 'TAX', 'LSTAT']]\n",
    "y = df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Initialize an instance of a Gradient Boosting Regression\n",
    "model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning algorithm to optimize its performance on a give dataset.\n",
    "\n",
    "To find the best set of hyperparameters for our model, we decided to use GridSearchCV. GridSearchCV (Grid Search Cross-Validation) searches through a manually specified subset of hyperparameter combinations to find the best set for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass ranges for Hyperparameters\n",
    "parameters = {'max_depth': [2, 3, 4],\n",
    "              'min_samples_split': [4, 5, 6],\n",
    "              'learning_rate': [0.05, 0.1, 0.2],\n",
    "              'n_estimators': [25, 50, 75]}  \n",
    "\n",
    "# Initialize a Grid Search object\n",
    "clf = GridSearchCV(estimator = model, param_grid = parameters, cv = 5)\n",
    "\n",
    "# Fit the Grid Search object to the train data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the optimal hyperparamer values, found by Grid Search CV\n",
    "best_depth = clf.best_estimator_.max_depth\n",
    "best_min_samples = clf.best_estimator_.min_samples_split\n",
    "best_learning_rate = clf.best_estimator_.learning_rate\n",
    "best_n_estimators = clf.best_estimator_.n_estimators\n",
    "\n",
    "print(f'Best depth: {best_depth}')\n",
    "print(f'Best minimum number of samples: {best_min_samples}')\n",
    "print(f'Best learning rate: {best_learning_rate}')\n",
    "print(f'Best number of estimators: {best_n_estimators}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE) is {mae:.4f}')\n",
    "print(f'Mean Squared Error (MSE) is {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE) is {mse**0.5:.4f}')\n",
    "print(f'R-squared is {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model - From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"This class defines the attributes of a tree node.\"\"\"\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, value = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"Define a decision tree regressor.\n",
    "    \n",
    "    Args:\n",
    "        min_samples (int, optional): The minimum number of samples required to split an\n",
    "        internal node. Defaults to 5.\n",
    "        max_depth (int, optional): The maximum depth of the tree. Defaults to 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples: int = 5, max_depth: int = 3):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        X = X.to_numpy()\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> list:\n",
    "        \"\"\"Predict the y values.\"\"\"\n",
    "        X = X.to_numpy()\n",
    "        return [self._transverse(x, self.root) for x in X]\n",
    "\n",
    "    def _transverse(self, x: np.array, node: Node) -> float:\n",
    "        \"\"\"Retrieve the value of y.\"\"\"\n",
    "        if node.value is None:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return self._transverse(x, node.left)\n",
    "            return self._transverse(x, node.right)\n",
    "        return node.value\n",
    "\n",
    "    def _grow_tree(self, X: np.array, y: np.array, depth: int = 0) -> Node:\n",
    "        \"\"\"Split the data into leaf nodes.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        if n_samples >= self.min_samples and depth <= self.max_depth:\n",
    "            index, value = self._best_split(X, y)\n",
    "            left_mask = X[:, index] <= value\n",
    "            right_mask = X[:, index] > value\n",
    "\n",
    "            left = self._grow_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right = self._grow_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "            return Node(feature = index, threshold = value, left = left, right = right)\n",
    "\n",
    "        return Node(value = self._leaf_node(y))\n",
    "\n",
    "    def _best_split(self, X: pd.DataFrame, y: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Find the best threshold and index to split the node on.\"\"\"\n",
    "        best_rss = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_i in range(n_features):\n",
    "            for threshold in np.unique(X[:, feature_i]):\n",
    "                left_mask = X[:, feature_i] <= threshold\n",
    "                right_mask = X[:, feature_i] > threshold\n",
    "                rss = self._rss(y, left_mask, right_mask)\n",
    "                if rss < best_rss:\n",
    "                    best_rss = rss\n",
    "                    best_feature = feature_i\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _rss(self, y: pd.DataFrame, left, right) -> float:\n",
    "        \"\"\"Calculate the Residual Sum of Squares (RSS).\"\"\"\n",
    "        y_left = y[left]\n",
    "        y_right = y[right]\n",
    "        rss_left = np.sum((y_left - np.mean(y_left)) ** 2)\n",
    "        rss_right = np.sum((y_right - np.mean(y_right)) ** 2)\n",
    "        return rss_left + rss_right\n",
    "\n",
    "    def _leaf_node(self, y: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate the value of a leaf node.\"\"\"\n",
    "        return np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchGradientBoostingRegressor():\n",
    "    \"\"\"Define a gradient boosting regressor.\"\"\"\n",
    "    def __init__(self, max_depth = 3, min_samples = 5, learning_rate = 0.1, n_estimators = 50):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = DecisionTree(min_samples = self.min_samples, max_depth = self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a gradient boosting regressor from the training set (X, y).\"\"\"\n",
    "        y_pred = np.copy(y)\n",
    "        for tree in self.trees:\n",
    "            tree.fit(X, y_pred)\n",
    "            new_y_pred = tree.predict(X)\n",
    "            y_pred = y_pred - np.multiply(self.learning_rate, new_y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the y values.\"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            new_y_pred = tree.predict(X)\n",
    "            y_pred += np.multiply(self.learning_rate, new_y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metrics\n",
    "We calculated a few the basic performance metrics from scratch to evaluate our model with\n",
    "including Mean Absolute Error, Mean Squared Error, and The sum of the squared residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_val(y, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
    "    return np.mean(abs(y - y_pred))\n",
    "\n",
    "def mse_val(y, y_pred):\n",
    "    \"\"\"Calculate Mean Square Error.\"\"\"\n",
    "    return np.mean(np.square(np.subtract(y, y_pred)))\n",
    "\n",
    "def r_square(y, y_pred):\n",
    "    \"\"\"Calculate R-Squared.\"\"\"\n",
    "    ssr = np.sum(np.square(np.subtract(y, y_pred)))\n",
    "    sst = np.sum(np.square(np.subtract(y, np.mean(y))))\n",
    "    return 1 - ssr/sst\n",
    "\n",
    "def performance_evaluation(y, y_pred):\n",
    "    \"\"\"Print all the performance metrics.\"\"\"\n",
    "    mae = mae_val(y, y_pred)\n",
    "    mse = mse_val(y, y_pred)\n",
    "    r2 = r_square(y, y_pred)\n",
    "\n",
    "    print(f'Mean absolute error is {mae:.4f}')\n",
    "    print(f'MSE (test): {mse:.4f}')\n",
    "    print(f'RMSE (test): {mse**0.5:.4f}')\n",
    "    print(f'R-squared is {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our version of the sklearn train_test_split method. We included a random seed so that you can reproduce the same random\n",
    "dataset for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y, train_size: float = 0.8, seed: int = None):\n",
    "    \"\"\"Split the dataset into train and tests sets.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    num_samples = len(X)\n",
    "    index = np.arange(num_samples)\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    X = X.loc[index]\n",
    "    y = y.loc[index]\n",
    "\n",
    "    split_i = int(num_samples * train_size)\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Fold\n",
    "\n",
    "The k-fold method is a tool used in hyperparameter tuning that allows you to split up the data into multiple subsets or \"folds\" and then treat each individual subset\n",
    "as the validation, or testing set. The other subsets are treated as the trainning set for the model to fit on. This method is useful as it reduces the variance and bias\n",
    "of the predicted target values. It also accounts for the variability of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(model, X, y, n_folds: int = 5, seed: int = None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    index = list(np.arange(len(X)))\n",
    "    np.random.shuffle(index)\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        marker = int(len(X)/n_folds)\n",
    "        testing_index = index[marker * i : marker * (i+1)]\n",
    "        training_index = index[:marker * i] + index[marker * (i+1):]\n",
    "        if i == (n_folds - 1):\n",
    "            testing_index += index[marker * (i+1):]\n",
    "            training_index = training_index[:marker* i]\n",
    "\n",
    "        X_train = X.loc[training_index]\n",
    "        X_test = X.loc[testing_index]\n",
    "        y_train = y.loc[training_index]\n",
    "        y_test = y.loc[testing_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r_square(y_test, y_pred)\n",
    "        results.append(r2)\n",
    "    \n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of Grid Search hyperparameter tuning method finds all of the possible combinations of the hyperparameters \n",
    "and then runs the model on all of the different combinations to find the optimal combination that produces the most positive\n",
    "results. There is also a input parameter for the number of folds to be used if you want to utilize k-fold cross validation. \n",
    "Using this function we found that the optimal paramters for our gradient boosting model were a minimum sample split size of 6, \n",
    "a maximum decision tree depth of 3, a learning rate of 0.2, and the number of tree estimators to be 50. These parameters gave us a\n",
    "r2 score of 0.86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(parameters, X, y, n_folds: int = 0, train_size: float = 0.8, seed: int = None):\n",
    "    X_train, X_test, y_train, y_test = train_test(X, y, train_size, seed)\n",
    "\n",
    "    best_r2 = 0\n",
    "    param_combinations = list(itertools.product(*parameters.values()))\n",
    "\n",
    "    for combo in param_combinations:\n",
    "        model = ScratchGradientBoostingRegressor(**dict(zip(parameters.keys(), combo)))\n",
    "\n",
    "        r2 = 0\n",
    "        if n_folds > 0:\n",
    "            r2 = k_fold(model, X, y, n_folds, seed)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2 = r_square(y_test, y_pred)\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_combo = combo\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "    return best_r2, best_combo, best_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2, 3, 4],\n",
    "              'min_samples': [4, 5, 6],\n",
    "              'learning_rate': [0.05, 0.1, 0.2],\n",
    "              'n_estimators': [25, 50, 75]} \n",
    "\n",
    "best_r2, best_params, y_pred = grid_search(parameters, X, y, n_folds = 0, train_size = 0.8, seed = 42)\n",
    "\n",
    "performance_evaluation(y_test, y_pred)\n",
    "sns.scatterplot(x = y_test, y = y_pred)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_evaluation(y_test, y_pred)\n",
    "\n",
    "sns.scatterplot(x = y_test, y = y_pred, color = '#14F278')\n",
    "\n",
    "plt.xlabel(\"Actual Values (y_test)\")\n",
    "plt.ylabel(\"Predicted Values (y_pred)\")\n",
    "plt.title(\"Scatter Plot of Actual vs. Predicted Values\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
